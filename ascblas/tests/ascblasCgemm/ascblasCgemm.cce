#ifdef __CCE_KT_TEST__
#define __aicore__
#else
#define __aicore__ [aicore]
#endif
// CAmodel 不需要头文件
#ifndef CAMODEL
#endif
#include "ascblas_kernel_utils.h"
using namespace fp32;

#if __DAV_C220_CUBE__
// CAmodel 需要将参数写入到tiling_para_gm中


__aicore__ __inline__ void ascblasSmatmul(
    ascblasOperation_t transA,
    ascblasOperation_t transB,
    int64_t M,
    int64_t N,
    int64_t K,
    __gm__ float * __restrict__ gm_A,
    int64_t lda,
    __gm__ float * __restrict__ gm_B,
    int64_t ldb,
    __gm__ float * __restrict__ gm_C,
    int64_t ldc,
    int64_t batchSize,
    int64_t M0,
    int64_t N0,
    int64_t K0,
    int64_t is_AIV_AIC_PIPE,
    __gm__ float * __restrict__ AIV_AIC_workspace
)
{
    auto L1_base_a = reinterpret_cast<__cbuf__ float *>((uintptr_t)0);            // 128 KB
    auto L1_base_b = reinterpret_cast<__cbuf__ float *>((uintptr_t)(128 * 1024)); // 128 KB

    auto L0A_base = reinterpret_cast<__ca__ float *>((uintptr_t)0);
    auto L0B_base = reinterpret_cast<__cb__ float *>((uintptr_t)0);
    auto L0C_base = reinterpret_cast<__cc__ float *>((uintptr_t)0);

    int64_t m_loop = (M + M0 - 1) / M0; // 在 M 方向分的核数
    int64_t n_loop = (N + N0 - 1) / N0; // 在 N 方向分的核数
    int64_t k_loop = (K + K0 - 1) / K0; // K 方向循环的次数
    int64_t loop = batchSize * m_loop * n_loop; // 总的核数

    int64_t loop_ping_flag = 1; // 控制loop循环的双循环
    int64_t k_loop_ping_flag = 1; // 控制k_loop循环的双循环

    set_flag(PIPE_FIX, PIPE_M, EVENT_ID0); // 同步L0C的使用
    set_flag(PIPE_FIX, PIPE_M, EVENT_ID1); // 同步L0C的使用

    set_flag(PIPE_MTE1, PIPE_MTE2, EVENT_ID0); // 同步L1_buf_a的使用
    set_flag(PIPE_MTE1, PIPE_MTE2, EVENT_ID1); // 同步L1_buf_a的使用
    set_flag(PIPE_MTE1, PIPE_MTE2, EVENT_ID2); // 同步L1_buf_b的使用
    set_flag(PIPE_MTE1, PIPE_MTE2, EVENT_ID3); // 同步L1_buf_b的使用
    
    set_flag(PIPE_M, PIPE_MTE1, EVENT_ID0); // 同步L1 -> (L0A | L0B)
    set_flag(PIPE_M, PIPE_MTE1, EVENT_ID1); // 同步L1 -> (L0A | L0B)

    // 循环遍历每个分块
    for (int64_t loop_idx = 0; loop_idx < loop; loop_idx++) {

        // 该循环计算L1上B分块(N0, K0)和A分块(K0, M0)的矩阵乘法并加到L0C上

        // 平均分配给物理核心，不是自己的任务就略过到下一项
        if (loop_idx % get_block_num() != get_block_idx()) {
            continue;
        }

        // 定义变量全部使用双缓冲
        auto L0C_buf = loop_ping_flag ? L0C_base + L0C_PINGPONG_BUFFER_LEN : L0C_base;
        auto LOOP_EVENT_ID = loop_ping_flag ? EVENT_ID0 : EVENT_ID1;

        int64_t batch_idx = loop_idx / (m_loop * n_loop);
        int64_t in_batch_idx = loop_idx % (m_loop * n_loop);
        int64_t m_idx;
        int64_t n_idx;

        // 让分块的id按照zZ的方式排列
        constexpr int64_t N_COL = 16;
        int64_t tile_block_loop = (n_loop + N_COL - 1) / N_COL;
        int64_t tile_block_idx = in_batch_idx / (N_COL * m_loop);
        int64_t in_tile_block_idx = in_batch_idx % (N_COL * m_loop);
        int64_t n_col = N_COL;
        if(tile_block_idx == tile_block_loop - 1) {
            n_col = n_loop - N_COL * tile_block_idx;
        }
        m_idx = in_tile_block_idx / n_col;
        n_idx = tile_block_idx * N_COL + in_tile_block_idx % n_col;
        // m_idx = in_batch_idx / n_loop;
        // n_idx = in_batch_idx % n_loop;
        
        // 计算分块的偏移量
        int64_t offset_a, offset_b;
        int64_t offset_c = batch_idx * ldc * N + m_idx * M0 + n_idx * N0 * ldc;
        // 计算分块的实际大小和向上16取整的大小
        int64_t m_actual = (m_idx == (m_loop - 1)) ? (M - m_idx * M0) : M0;
        int64_t n_actual = (n_idx == (n_loop - 1)) ? (N - n_idx * N0) : N0;
        int64_t m_round = ROUND(m_actual, 16);
        int64_t n_round = ROUND(n_actual, 16);

        // 计算L0AB中k方向的最大能放多少个数
        int64_t mn_max = m_round > n_round ? m_round : n_round;
        int64_t L0AB_K0 = L0AB_PINGPONG_BUFFER_LEN / mn_max / 16 * 16;

        for (int64_t k_idx = 0; k_idx < k_loop; k_idx++) {
            // 计算分块的地址位置
            if(transA != ASCBLAS_OP_N) {
                offset_a = batch_idx * M * lda + k_idx * K0 + m_idx * M0 * lda;
            } else {
                offset_a = batch_idx * lda * K + m_idx * M0 + k_idx * K0 * lda;
            }

            if(transB != ASCBLAS_OP_N) {
                offset_b = batch_idx * K * ldb + n_idx * N0 + k_idx * K0 * ldb;
            } else {
                offset_b = batch_idx * ldb * N + k_idx * K0 + n_idx * N0 * ldb;
            }

            // 计算分块的实际大小和向上16取整的大小
            int64_t k_actual = (k_idx == (k_loop - 1)) ? (K - k_idx * K0) : K0;
            int64_t k_round = ROUND(k_actual, 16);
            int64_t L0AB_k_loop = (k_actual + L0AB_K0 - 1) / L0AB_K0;

            auto L1_buf_a = k_loop_ping_flag ? L1_base_a : L1_base_a + L1_PINGPONG_BUFFER_LEN;
            auto L1_buf_b = k_loop_ping_flag ? L1_base_b : L1_base_b + L1_PINGPONG_BUFFER_LEN;
            auto K_LOOP_EVENT_ID = k_loop_ping_flag ? EVENT_ID0 : EVENT_ID1;

            // *** load matrix A to L1
            wait_flag(PIPE_MTE1, PIPE_MTE2, K_LOOP_EVENT_ID); // 控制 L1_buf_a 的使用
            if (transA != ASCBLAS_OP_N) {
                ascblas_matrix_gm2cbuf_ND2nZ(L1_buf_a, gm_A + offset_a, K0, M0, k_actual, m_actual, lda);
            } else {
                ascblas_matrix_gm2cbuf_ND2nN(L1_buf_a, gm_A + offset_a, M0, K0, m_actual, k_actual, lda);
            }
            set_flag(PIPE_MTE2, PIPE_MTE1, K_LOOP_EVENT_ID); // L1_buf_a 读取完毕，可以开始进行L1 -> L0B的拷贝了
            

            // *** load matrix B to L1
            wait_flag(PIPE_MTE1, PIPE_MTE2, K_LOOP_EVENT_ID + 2); // 控制 L1_buf_b 的使用
            if (transB != ASCBLAS_OP_N) {
                ascblas_matrix_gm2cbuf_ND2nN(L1_buf_b, gm_B + offset_b, N0, K0, n_actual, k_actual, ldb);
            } else {
                ascblas_matrix_gm2cbuf_ND2nZ(L1_buf_b, gm_B + offset_b, K0, N0, k_actual, n_actual, ldb);
            }

            set_flag(PIPE_MTE2, PIPE_MTE1, K_LOOP_EVENT_ID + 2); // L1_buf_b 读取完毕，可以开始进行L1 -> L0A的拷贝了

            for (int L0AB_k_idx = 0; L0AB_k_idx < L0AB_k_loop; L0AB_k_idx++) {
                // 该循环计算L0AB上B分块(N0, L0AB_K0)和A分块(L0AB_K0, M0)的矩阵乘法并加到L0C上
                int64_t L0AB_k_round = (L0AB_k_idx < L0AB_k_loop - 1) ? L0AB_K0 : k_round - L0AB_k_idx * L0AB_K0;
                int64_t L0AB_k_actual = (L0AB_k_idx < L0AB_k_loop - 1) ? L0AB_K0 : k_actual - L0AB_k_idx * L0AB_K0;
            
                auto mte1_mad_ping_flag = 1 - L0AB_k_idx % 2;
                auto mte1_mad_event_id = mte1_mad_ping_flag ? EVENT_ID0 : EVENT_ID1;
                auto L0A_buf = L0A_base + (L0AB_k_idx % 2) * L0AB_PINGPONG_BUFFER_LEN;
                auto L0B_buf = L0B_base + (L0AB_k_idx % 2) * L0AB_PINGPONG_BUFFER_LEN;
            
                // *** load matrix A from L1 to L0B
                if (L0AB_k_idx == 0) {
                    wait_flag(PIPE_MTE2, PIPE_MTE1, K_LOOP_EVENT_ID); // 等待gm -> L1结束，在读取到L0B上
                }
                wait_flag(PIPE_M, PIPE_MTE1, mte1_mad_event_id); // 当CUBE计算结束，才能拷贝到L0AB上
                if (transA != ASCBLAS_OP_N) {
                    auto L1_src_a = L1_buf_a + L0AB_k_idx * L0AB_K0 * M0;
                    for (int i = 0; i < L0AB_k_round / CUBE_K0; i++) {
                        load_cbuf_to_cb(
                            L0B_buf + i * m_round * CUBE_K0,
                            L1_src_a + i * CUBE_K0 * M0,
                            0,
                            m_round / CUBE_M0,
                            1,
                            0,
                            0,
                            false,
                            inc
                        );
                    }
                } else {
                    auto L1_src_a = L1_buf_a + L0AB_k_idx * L0AB_K0 * M0;
                    for (int i = 0; i < m_round / CUBE_M0; i++) {
                        load_cbuf_to_cb_transpose(
                            L0B_buf + i * CUBE_MATRIX_SIZE,
                            L1_src_a + i * 2 * CUBE_MATRIX_SIZE,
                            0,
                            L0AB_k_round / (2 * CUBE_K0),
                            M0 / CUBE_M0,
                            2 * m_round / CUBE_M0 - 1,
                            inc,
                            m_round / CUBE_M0 - 1
                        );
                    }
                }

                if (L0AB_k_idx == L0AB_k_loop - 1) { // L1上的数据已经读取完毕，可以进行下一次 GM -> L1 了
                    set_flag(PIPE_MTE1, PIPE_MTE2, K_LOOP_EVENT_ID);
                }

                // *** load matrix B from L1 to L0B
                if (L0AB_k_idx == 0) {
                    wait_flag(PIPE_MTE2, PIPE_MTE1, K_LOOP_EVENT_ID + 2); // 等待gm -> L1结束，在读取到L0B上
                }
                if (transB != ASCBLAS_OP_N) {
                    auto L1_src_b = L1_buf_b + L0AB_k_idx * L0AB_K0 * N0; // 当CUBE计算结束，才能拷贝到L0AB上
                    for (int i = 0; i < n_round / CUBE_N0; i++) {
                        load_cbuf_to_ca_transpose(
                            L0A_buf + i * L0AB_k_round * CUBE_N0,
                            L1_src_b + i * 2 * CUBE_MATRIX_SIZE,
                            0,
                            L0AB_k_round / (2 * CUBE_K0),
                            N0 / CUBE_N0,
                            1,
                            inc,
                            0
                        );
                    }
                } else {
                    auto L1_src_b = L1_buf_b + L0AB_k_idx * L0AB_K0 * N0;
                    for (int i = 0; i < n_round / CUBE_N0; i++) {
                        load_cbuf_to_ca(
                            L0A_buf + i * L0AB_k_round * CUBE_N0,
                            L1_src_b + i * CUBE_MATRIX_SIZE,
                            0,
                            L0AB_k_round / CUBE_K0,
                            N0 / CUBE_N0,
                            0,
                            0,
                            false,
                            inc
                        );
                    }
                }
                if (L0AB_k_idx == L0AB_k_loop - 1) { // L1上的数据已经读取完毕，可以进行下一次 GM -> L1 了
                    set_flag(PIPE_MTE1, PIPE_MTE2, K_LOOP_EVENT_ID + 2);
                }

                set_flag(PIPE_MTE1, PIPE_M, mte1_mad_event_id);
                wait_flag(PIPE_MTE1, PIPE_M, mte1_mad_event_id);

                bool init_c = (k_idx == 0 && L0AB_k_idx == 0); // 第一次循环直接赋值即可
                if (init_c) { // 同步L0C的写入和CUBE的计算
                    wait_flag(PIPE_FIX, PIPE_M, LOOP_EVENT_ID);
                }
                mad(L0C_buf,
                    L0A_buf,
                    L0B_buf,
                    n_round,
                    L0AB_k_actual,
                    m_round,
                    0,
                    1,
                    0,
                    init_c
                );
                
                pipe_barrier(PIPE_M);
                set_flag(PIPE_M, PIPE_MTE1, mte1_mad_event_id); // CUBE计算完毕，可以进行 L1 -> L0AB 的拷贝了
            }
            k_loop_ping_flag = 1 - k_loop_ping_flag; // 更换标记做双缓存
        }

        // CUBE计算完毕，存回到GM
        set_flag(PIPE_M, PIPE_FIX, LOOP_EVENT_ID);
        wait_flag(PIPE_M, PIPE_FIX, LOOP_EVENT_ID);

        // copy from L0C to gm
        // 如果需要乘alpha加C拷贝到 AIV_AIC_workspace 上便于AIV获取
        if (is_AIV_AIC_PIPE) {
            wait_flag_dev(loop_ping_flag + 2); // 等待AIV上 AIV_AIC_workspace 使用完毕
            copy_matrix_cc_to_gm(
                AIV_AIC_workspace + (get_block_idx() * 2 + loop_ping_flag) * M0 * N0,
                L0C_buf,
                0,
                m_actual,
                n_actual,
                M0,
                n_round,
                0,
                NoQuant,
                0,
                false,
                true
            );
            ffts_cross_core_sync(PIPE_FIX, GET_FFST_MSG(2, loop_ping_flag)); // AIC已经将AB写入到AIV_AIC_workspace中，AIV可以开始读取
        } else {
            // 不需要乘alpha加C直接写入到GM
            copy_matrix_cc_to_gm(
                gm_C + offset_c,
                L0C_buf,
                0,
                m_actual,
                n_actual,  
                ldc,   
                n_round,
                0,
                NoQuant,
                0,
                false,
                true
            );
        }

        loop_ping_flag = 1 - loop_ping_flag;

        set_flag(PIPE_FIX, PIPE_M, LOOP_EVENT_ID);
    }
    if (is_AIV_AIC_PIPE) {
        wait_flag_dev(0 + 2); // 处理最后一次AIV的set同步
        wait_flag_dev(1 + 2); // 处理最后一次AIV的set同步
    }

    // 匹配最后一次的set
    wait_flag(PIPE_M, PIPE_MTE1, EVENT_ID0);
    wait_flag(PIPE_M, PIPE_MTE1, EVENT_ID1);

    wait_flag(PIPE_MTE1, PIPE_MTE2, EVENT_ID0);
    wait_flag(PIPE_MTE1, PIPE_MTE2, EVENT_ID1);
    wait_flag(PIPE_MTE1, PIPE_MTE2, EVENT_ID2);
    wait_flag(PIPE_MTE1, PIPE_MTE2, EVENT_ID3);

    wait_flag(PIPE_FIX, PIPE_M, EVENT_ID0);
    wait_flag(PIPE_FIX, PIPE_M, EVENT_ID1);
}

#ifdef CAMODEL
extern "C" __global__ __aicore__ void ascblasCgemm_kernel_mix_aic(
    __gm__ uint8_t * __restrict__ ffts_addr,
    __gm__ float * __restrict__ d_A,
    __gm__ float * __restrict__ d_B,
    __gm__ float * __restrict__ d_A_r,
    __gm__ float * __restrict__ d_A_i,
    __gm__ float * __restrict__ d_B_r,
    __gm__ float * __restrict__ d_B_i,
    __gm__ float * __restrict__ d_C_rr,
    __gm__ float * __restrict__ d_C_ri,
    __gm__ float * __restrict__ d_C_ir,
    __gm__ float * __restrict__ d_C_ii,
    __gm__ float * __restrict__ d_C,
    __gm__ uint32_t * __restrict__ tiling_para_gm
    )
#else
extern "C" __global__ __aicore__ void ascblasCgemm_kernel_mix_aic(
    ascblasOperation_t transA,
    ascblasOperation_t transB,
    int64_t M, int64_t N, int64_t K,
    ascComplex alpha,
    __gm__ float * __restrict__ d_A, int64_t lda,
    __gm__ float * __restrict__ d_B, int64_t ldb,
    ascComplex beta,
    __gm__ float * __restrict__ d_C, int64_t ldc,
    int64_t lda_pad, int64_t ldb_pad,
    __gm__ float * __restrict__ d_A_r,
    __gm__ float * __restrict__ d_A_i,
    __gm__ float * __restrict__ d_B_r,
    __gm__ float * __restrict__ d_B_i,
    __gm__ float * __restrict__ d_C_rr,
    __gm__ float * __restrict__ d_C_ri,
    __gm__ float * __restrict__ d_C_ir,
    __gm__ float * __restrict__ d_C_ii,
    __gm__ uint8_t * __restrict__ ffts_addr)
#endif
{
    set_ffts_base_addr((uint64_t)ffts_addr);
    set_padding(0);
    set_atomic_none();
    set_nd_para(0x1);
// CAmodel 从tiling_para_gm解析出参数
#ifdef CAMODEL
    // get tiling args
    auto tiling_para = reinterpret_cast<__gm__ int32_t *>(tiling_para_gm);
    ascblasOperation_t transA = tiling_para[0] == 0 ? ASCBLAS_OP_N : (tiling_para[0] == 1?ASCBLAS_OP_T:ASCBLAS_OP_C);
    ascblasOperation_t transB = tiling_para[1] == 0 ? ASCBLAS_OP_N : (tiling_para[1] == 1?ASCBLAS_OP_T:ASCBLAS_OP_C);
    int64_t M = tiling_para[2];
    int64_t N = tiling_para[3];
    int64_t K = tiling_para[4];
    ascComplex alpha {static_cast<float>(tiling_para[5]), static_cast<float>(tiling_para[6])};
    int64_t lda = tiling_para[7];
    int64_t ldb = tiling_para[8];
    ascComplex beta {static_cast<float>(tiling_para[9]), static_cast<float>(tiling_para[10])};
    int64_t ldc = tiling_para[11];
    int64_t lda_pad = tiling_para[12];
    int64_t ldb_pad = tiling_para[13];
#endif
    wait_flag_dev(0);

    ascblasSmatmul(transA, transB, M, N, K, d_A_r, lda_pad, d_B_r, ldb_pad, d_C_rr, ldc, 1, 128, 128, 128, 0, nullptr);
    ascblasSmatmul(transA, transB, M, N, K, d_A_r, lda_pad, d_B_i, ldb_pad, d_C_ri, ldc, 1, 128, 128, 128, 0, nullptr);
    ascblasSmatmul(transA, transB, M, N, K, d_A_i, lda_pad, d_B_r, ldb_pad, d_C_ir, ldc, 1, 128, 128, 128, 0, nullptr);
    ascblasSmatmul(transA, transB, M, N, K, d_A_i, lda_pad, d_B_i, ldb_pad, d_C_ii, ldc, 1, 128, 128, 128, 0, nullptr);
    
    ffts_cross_core_sync(PIPE_FIX, GET_FFST_MSG(0, 1));
    wait_flag_dev(1);
    ffts_cross_core_sync(PIPE_FIX, GET_FFST_MSG(2, 1));
    
    pipe_barrier(PIPE_ALL);
}
#endif
#if __DAV_C220_VEC__

constexpr int BLOCK_SIZE = 32/sizeof(float);
__aicore__ __inline__ void splitMatrix(
    __gm__ float *dst_r, __gm__ float *dst_i,
    __gm__ float *src, 
    int64_t M, int64_t N,
    int64_t dst_pad, int64_t src_pad
    )
{
    int64_t NUM_PER_REPEAT = 12*1024;
    auto complex_buf = reinterpret_cast<__ubuf__ float*>((uintptr_t)0);
    auto real_buf = reinterpret_cast<__ubuf__ float*>((uintptr_t)(NUM_PER_REPEAT * 2 * sizeof(float)));
    auto imag_buf = reinterpret_cast<__ubuf__ float*>((uintptr_t)(NUM_PER_REPEAT * 3 * sizeof(float)));
    int64_t v_block_num = get_block_num()*get_subblockdim();
    int64_t v_id = get_block_idx()*get_subblockdim()+get_subblockid();

    int64_t m_repeats = (M + NUM_PER_REPEAT - 1) / NUM_PER_REPEAT;
    int64_t repeats = N*m_repeats;
    int64_t repeats_per_core = repeats/v_block_num + int(v_id < repeats%v_block_num);

    set_flag(PIPE_V, PIPE_MTE2, EVENT_ID0);
    set_flag(PIPE_MTE3, PIPE_V, EVENT_ID0);
    set_flag(PIPE_MTE3, PIPE_V, EVENT_ID1);
    for (int64_t pi = 0; pi < repeats_per_core; pi++) {
        int64_t i = pi*v_block_num+v_id;
        int64_t c_id = i/m_repeats;
        int64_t r_id = i%m_repeats;
        int64_t len = NUM_PER_REPEAT;
        if (r_id+1 == m_repeats) {
            len = M-r_id*NUM_PER_REPEAT;
        }
        int64_t n_blocks = (len + BLOCK_SIZE - 1) / BLOCK_SIZE;
        int64_t two_n_blocks = (len * 2 + BLOCK_SIZE - 1) / BLOCK_SIZE;

        wait_flag(PIPE_V, PIPE_MTE2, EVENT_ID0);
        copy_gm_to_ubuf(complex_buf, src + (c_id*src_pad+r_id*NUM_PER_REPEAT) * 2, 0, 1, two_n_blocks, 0, 0);
        set_flag(PIPE_MTE2, PIPE_V, EVENT_ID0);
        wait_flag(PIPE_MTE2, PIPE_V, EVENT_ID0);
        wait_flag(PIPE_MTE3, PIPE_V, EVENT_ID0);
        vreducev2(
            reinterpret_cast<__ubuf__ uint32_t *>(real_buf),
            reinterpret_cast<__ubuf__ uint32_t *>(complex_buf),
            nullptr,
            (two_n_blocks + 7) / 8, // repeat
            1,                  // src0BlockStride
            1,                  // patternMode, 101010...10
            8,                  // src0RepeatStride
            8                   // src1RepeatStride
        );
        set_flag(PIPE_V, PIPE_MTE3, EVENT_ID0);
        wait_flag(PIPE_MTE3, PIPE_V, EVENT_ID1);
        vreducev2(
            reinterpret_cast<__ubuf__ uint32_t *>(imag_buf),
            reinterpret_cast<__ubuf__ uint32_t *>(complex_buf),
            nullptr,
            (two_n_blocks + 7) / 8, // repeatb
            1,                  // src0BlockStride
            2,                  // patternMode, 101010...10
            8,                  // src0RepeatStride
            8                   // src1RepeatStride
        );
        set_flag(PIPE_V, PIPE_MTE3, EVENT_ID1);
        set_flag(PIPE_V, PIPE_MTE2, EVENT_ID0);
        wait_flag(PIPE_V, PIPE_MTE3, EVENT_ID0);
        copy_ubuf_to_gm(dst_r + c_id*dst_pad+r_id* NUM_PER_REPEAT, real_buf, 0, 1, n_blocks, 0, 0);
        set_flag(PIPE_MTE3, PIPE_V, EVENT_ID0);
        wait_flag(PIPE_V, PIPE_MTE3, EVENT_ID1);
        copy_ubuf_to_gm(dst_i + c_id*dst_pad+r_id* NUM_PER_REPEAT, imag_buf, 0, 1, n_blocks, 0, 0);
        set_flag(PIPE_MTE3, PIPE_V, EVENT_ID1);
    }
    wait_flag(PIPE_V, PIPE_MTE2, EVENT_ID0);
    wait_flag(PIPE_MTE3, PIPE_V, EVENT_ID0);
    wait_flag(PIPE_MTE3, PIPE_V, EVENT_ID1);
}
__aicore__ __inline__ void ascblasCgemmPre(
    ascblasOperation_t transA,
    ascblasOperation_t transB,
    int64_t M, int64_t N, int64_t K,
    ascComplex alpha,
    __gm__ float * __restrict__ d_A, int64_t lda,
    __gm__ float * __restrict__ d_B, int64_t ldb,
    ascComplex beta,
    __gm__ float * __restrict__ d_C, int64_t ldc,
    int64_t lda_pad, int64_t ldb_pad,
    __gm__ float * __restrict__ d_A_r,
    __gm__ float * __restrict__ d_A_i,
    __gm__ float * __restrict__ d_B_r,
    __gm__ float * __restrict__ d_B_i,
    __gm__ float * __restrict__ d_C_rr,
    __gm__ float * __restrict__ d_C_ri,
    __gm__ float * __restrict__ d_C_ir,
    __gm__ float * __restrict__ d_C_ii,
    __gm__ uint8_t * __restrict__ ffts_addr) 
{
    if (transA == ASCBLAS_OP_N)
        splitMatrix(d_A_r, d_A_i, d_A, M, K, lda_pad, lda);
    else
        splitMatrix(d_A_r, d_A_i, d_A, K, M, lda_pad, lda);
    if (transB == ASCBLAS_OP_N)
        splitMatrix(d_B_r, d_B_i, d_B, K, N, ldb_pad, ldb);
    else
        splitMatrix(d_B_r, d_B_i, d_B, N, K, ldb_pad, ldb);
}

__aicore__ __inline__ void mulSComplex(
    __ubuf__ float *src_r, __ubuf__ float *src_i,
    __ubuf__ float *tmp_r, __ubuf__ float *tmp_i,
    int64_t n_blocks,
    ascComplex beta
    )
{
    vmuls(tmp_r, src_r, beta.real, (n_blocks+7)/8, 1, 1, 8, 8);
    vmuls(tmp_i, src_i, beta.imag, (n_blocks+7)/8, 1, 1, 8, 8);
    pipe_barrier(PIPE_V);
    vsub(tmp_r, tmp_r, tmp_i, (n_blocks+7)/8, 1, 1, 1, 8, 8, 8);
    vmuls(tmp_i, src_r, beta.imag, (n_blocks+7)/8, 1, 1, 8, 8);
    pipe_barrier(PIPE_V);
    copy_ubuf_to_ubuf(src_r, tmp_r, 0, 1, n_blocks, 0, 0);
    pipe_barrier(PIPE_V);
    vmuls(tmp_r, src_i, beta.real, (n_blocks+7)/8, 1, 1, 8, 8);
    pipe_barrier(PIPE_V);
    vadd(tmp_i, tmp_i, tmp_r, (n_blocks+7)/8, 1, 1, 1, 8, 8, 8);
    pipe_barrier(PIPE_V);
    copy_ubuf_to_ubuf(src_i, tmp_i, 0, 1, n_blocks, 0, 0);
}

__aicore__ __inline__ void ascblasCgemmFinal(
    ascblasOperation_t transA,
    ascblasOperation_t transB,
    int64_t M, int64_t N, int64_t K,
    ascComplex alpha,
    __gm__ float * __restrict__ d_A, int64_t lda,
    __gm__ float * __restrict__ d_B, int64_t ldb,
    ascComplex beta,
    __gm__ float * __restrict__ d_C, int64_t ldc,
    int64_t lda_pad, int64_t ldb_pad,
    __gm__ float * __restrict__ d_A_r,
    __gm__ float * __restrict__ d_A_i,
    __gm__ float * __restrict__ d_B_r,
    __gm__ float * __restrict__ d_B_i,
    __gm__ float * __restrict__ d_C_rr,
    __gm__ float * __restrict__ d_C_ri,
    __gm__ float * __restrict__ d_C_ir,
    __gm__ float * __restrict__ d_C_ii,
    __gm__ uint8_t * __restrict__ ffts_addr) 
{
    constexpr int64_t NUM_PER_REPEAT = 6*1024;
    auto complex_buf = reinterpret_cast<__ubuf__ float*>((uintptr_t)0);
    auto complex_buf_real = reinterpret_cast<__ubuf__ float*>((uintptr_t)0);
    auto complex_buf_imag = reinterpret_cast<__ubuf__ float*>((uintptr_t)NUM_PER_REPEAT * 1 * sizeof(float));
    auto real_buf = reinterpret_cast<__ubuf__ float*>((uintptr_t)(NUM_PER_REPEAT * 2 * sizeof(float)));
    auto imag_buf = reinterpret_cast<__ubuf__ float*>((uintptr_t)(NUM_PER_REPEAT * 3 * sizeof(float)));
    auto mask_buf = reinterpret_cast<__ubuf__ uint32_t*>((uintptr_t)(NUM_PER_REPEAT * 4 * sizeof(float)));
    auto real_tbuf = reinterpret_cast<__ubuf__ float*>((uintptr_t)(NUM_PER_REPEAT * 6 * sizeof(float)));
    auto imag_tbuf = reinterpret_cast<__ubuf__ float*>((uintptr_t)(NUM_PER_REPEAT * 7 * sizeof(float)));
    constexpr int64_t BYTE_PER_BLOCK_GROUP = 256;   // 每次gather最多可以处理的Byte数
    constexpr int64_t NUM_BLOCK_GROUP = 2 * NUM_PER_REPEAT*sizeof(float) / BYTE_PER_BLOCK_GROUP; 
    int v_block_num = get_block_num()*get_subblockdim();
    int v_id = get_block_idx()*get_subblockdim()+get_subblockid();
    int64_t m_repeats = (M+NUM_PER_REPEAT-1)/NUM_PER_REPEAT;
    int64_t repeats = N*m_repeats;
    int repeats_per_core = repeats/v_block_num + int(v_id < repeats%v_block_num);
    auto real_base = (uintptr_t)(NUM_PER_REPEAT * 2 * sizeof(float));
    auto img_base = (uintptr_t)(NUM_PER_REPEAT * 3 * sizeof(float));
    int k = 0;
    for (int j = 0; j < NUM_PER_REPEAT; j++) {
        mask_buf[k++] = real_base + j * sizeof(float);
        mask_buf[k++] = img_base + j * sizeof(float);
    }
    pipe_barrier(PIPE_ALL);
    set_flag(PIPE_MTE3, PIPE_MTE2, EVENT_ID0);
    for (int64_t pi = 0; pi < repeats_per_core; pi++) {
        int64_t i = pi*v_block_num+v_id;
        int64_t c_id = i/m_repeats;
        int64_t r_id = i%m_repeats;
        int64_t len = NUM_PER_REPEAT;
        int64_t gather_repeat = NUM_BLOCK_GROUP;
        if (r_id+1 == m_repeats) {
            len = M-r_id*NUM_PER_REPEAT;
            gather_repeat = (len * 2 * sizeof(float) + BYTE_PER_BLOCK_GROUP - 1) / BYTE_PER_BLOCK_GROUP ;   // 一次gather调用处理的Byte数
        }
        // pipe_barrier(PIPE_ALL);
        int64_t n_blocks = (len + BLOCK_SIZE - 1) / BLOCK_SIZE;
        int64_t two_n_blocks = (len * 2 + BLOCK_SIZE - 1) / BLOCK_SIZE;
        wait_flag(PIPE_MTE3, PIPE_MTE2, EVENT_ID0);
        copy_gm_to_ubuf(complex_buf, d_C + (c_id*ldc+r_id*NUM_PER_REPEAT) * 2, 0, 1, two_n_blocks, 0, 0);
        set_flag(PIPE_MTE2, PIPE_V, EVENT_ID0);
        wait_flag(PIPE_MTE2, PIPE_V, EVENT_ID0);
        vreducev2(
            reinterpret_cast<__ubuf__ uint32_t *>(real_buf),
            reinterpret_cast<__ubuf__ uint32_t *>(complex_buf),
            nullptr,
            (two_n_blocks + 7) / 8, // repeat
            1,                  // src0BlockStride
            1,                  // patternMode, 101010...10
            8,                  // src0RepeatStride
            8                   // src1RepeatStride
        );
        vreducev2(
            reinterpret_cast<__ubuf__ uint32_t *>(imag_buf),
            reinterpret_cast<__ubuf__ uint32_t *>(complex_buf),
            nullptr,
            (two_n_blocks + 7) / 8, // repeatb
            1,                  // src0BlockStride
            2,                  // patternMode, 101010...10
            8,                  // src0RepeatStride
            8                   // src1RepeatStride
        );

        pipe_barrier(PIPE_V);
        // beta*C
        mulSComplex(real_buf, imag_buf, real_tbuf, imag_tbuf, n_blocks, beta);

        set_flag(PIPE_V, PIPE_MTE2, EVENT_ID1);
        wait_flag(PIPE_V, PIPE_MTE2, EVENT_ID1);
        copy_gm_to_ubuf(complex_buf_real, d_C_rr + c_id*ldc+r_id*NUM_PER_REPEAT, 0, 1, n_blocks, 0, 0);
        copy_gm_to_ubuf(complex_buf_imag, d_C_ri + c_id*ldc+r_id*NUM_PER_REPEAT, 0, 1, n_blocks, 0, 0);
        copy_gm_to_ubuf(imag_tbuf, d_C_ir + c_id*ldc+r_id*NUM_PER_REPEAT, 0, 1, n_blocks, 0, 0);
        copy_gm_to_ubuf(real_tbuf, d_C_ii + c_id*ldc+r_id*NUM_PER_REPEAT, 0, 1, n_blocks, 0, 0);
        set_flag(PIPE_MTE2, PIPE_V, EVENT_ID1);
        wait_flag(PIPE_MTE2, PIPE_V, EVENT_ID1);
        if ((transA==ASCBLAS_OP_C) != (transB==ASCBLAS_OP_C))
            vadd(complex_buf_real, complex_buf_real, real_tbuf, (n_blocks+7)/8, 1, 1, 1, 8, 8, 8);
        else
            vsub(complex_buf_real, complex_buf_real, real_tbuf, (n_blocks+7)/8, 1, 1, 1, 8, 8, 8);
        if (transB==ASCBLAS_OP_C)
            vmuls(complex_buf_imag, complex_buf_imag, -1.0f, (n_blocks+7)/8, 1, 1, 8, 8);
        pipe_barrier(PIPE_V);

        if (transA==ASCBLAS_OP_C)
            vsub(complex_buf_imag, complex_buf_imag, imag_tbuf, (n_blocks+7)/8, 1, 1, 1, 8, 8, 8);
        else
            vadd(complex_buf_imag, complex_buf_imag, imag_tbuf, (n_blocks+7)/8, 1, 1, 1, 8, 8, 8);
        pipe_barrier(PIPE_V);
        // alpha*A*B
        mulSComplex(complex_buf_real, complex_buf_imag, real_tbuf, imag_tbuf, n_blocks, alpha);

        // +beta*C
        pipe_barrier(PIPE_V);
        vadd(real_buf, real_buf, complex_buf_real, (n_blocks+7)/8, 1, 1, 1, 8, 8, 8);
        vadd(imag_buf, imag_buf, complex_buf_imag, (n_blocks+7)/8, 1, 1, 1, 8, 8, 8);

        pipe_barrier(PIPE_V);
        vgather(
            reinterpret_cast<__ubuf__ uint32_t *>(complex_buf),
            reinterpret_cast<__ubuf__ uint32_t *>(mask_buf),
            0,              // offsetAddr
            8,              // dstRepeatStride，以32Byte为单位，1个block
            gather_repeat   // repeat，以256Byte为单位，8个block
        );
        set_flag(PIPE_V, PIPE_MTE3, EVENT_ID0);
        wait_flag(PIPE_V, PIPE_MTE3, EVENT_ID0);
        copy_ubuf_to_gm_align_b32(
                    d_C + (c_id*ldc+r_id*NUM_PER_REPEAT) * 2,
                    complex_buf,
                    0,
                    1,
                    len*2 * sizeof(float),
                    0,
                    0,
                    0,
                    0
                );
        set_flag(PIPE_MTE3, PIPE_MTE2, EVENT_ID0);
    }
    wait_flag(PIPE_MTE3, PIPE_MTE2, EVENT_ID0);
}
#ifdef CAMODEL
extern "C" __global__ __aicore__ void ascblasCgemm_kernel_mix_aiv(
    __gm__ uint8_t * __restrict__ ffts_addr,
    __gm__ float * __restrict__ d_A,
    __gm__ float * __restrict__ d_B,
    __gm__ float * __restrict__ d_A_r,
    __gm__ float * __restrict__ d_A_i,
    __gm__ float * __restrict__ d_B_r,
    __gm__ float * __restrict__ d_B_i,
    __gm__ float * __restrict__ d_C_rr,
    __gm__ float * __restrict__ d_C_ri,
    __gm__ float * __restrict__ d_C_ir,
    __gm__ float * __restrict__ d_C_ii,
    __gm__ float * __restrict__ d_C,
    __gm__ uint32_t * __restrict__ tiling_para_gm
    )
#else
extern "C" __global__ __aicore__ void ascblasCgemm_kernel_mix_aiv(
    ascblasOperation_t transA,
    ascblasOperation_t transB,
    int64_t M, int64_t N, int64_t K,
    ascComplex alpha,
    __gm__ float * __restrict__ d_A, int64_t lda,
    __gm__ float * __restrict__ d_B, int64_t ldb,
    ascComplex beta,
    __gm__ float * __restrict__ d_C, int64_t ldc,
    int64_t lda_pad, int64_t ldb_pad,
    __gm__ float * __restrict__ d_A_r,
    __gm__ float * __restrict__ d_A_i,
    __gm__ float * __restrict__ d_B_r,
    __gm__ float * __restrict__ d_B_i,
    __gm__ float * __restrict__ d_C_rr,
    __gm__ float * __restrict__ d_C_ri,
    __gm__ float * __restrict__ d_C_ir,
    __gm__ float * __restrict__ d_C_ii,
    __gm__ uint8_t * __restrict__ ffts_addr)
#endif
{
    set_atomic_none();
    set_mask_norm();
    set_ffts_base_addr((uint64_t)ffts_addr);
    set_vector_mask((uint64_t)-1, (uint64_t)-1);
#ifdef CAMODEL
    // get tiling args
    auto tiling_para = reinterpret_cast<__gm__ int32_t *>(tiling_para_gm);
    ascblasOperation_t transA = tiling_para[0] == 0 ? ASCBLAS_OP_N : (tiling_para[0] == 1?ASCBLAS_OP_T:ASCBLAS_OP_C);
    ascblasOperation_t transB = tiling_para[1] == 0 ? ASCBLAS_OP_N : (tiling_para[1] == 1?ASCBLAS_OP_T:ASCBLAS_OP_C);
    int64_t M = tiling_para[2];
    int64_t N = tiling_para[3];
    int64_t K = tiling_para[4];
    ascComplex alpha {static_cast<float>(tiling_para[5]), static_cast<float>(tiling_para[6])};
    int64_t lda = tiling_para[7];
    int64_t ldb = tiling_para[8];
    ascComplex beta {static_cast<float>(tiling_para[9]), static_cast<float>(tiling_para[10])};
    int64_t ldc = tiling_para[11];
    int64_t lda_pad = tiling_para[12];
    int64_t ldb_pad = tiling_para[13];
#endif
    ascblasCgemmPre(transA, transB, M, N, K, alpha, d_A, lda, d_B, ldb, beta, d_C, ldc, lda_pad, ldb_pad, d_A_r, d_A_i, d_B_r, d_B_i, d_C_rr, d_C_ri, d_C_ir, d_C_ii, ffts_addr); 
    ffts_cross_core_sync(PIPE_MTE3, GET_FFST_MSG(0, 0));
    wait_flag_dev(0);
    ffts_cross_core_sync(PIPE_MTE3, GET_FFST_MSG(2, 0));
    wait_flag_dev(1);
    ascblasCgemmFinal(transA, transB, M, N, K, alpha, d_A, lda, d_B, ldb, beta, d_C, ldc, lda_pad, ldb_pad, d_A_r, d_A_i, d_B_r, d_B_i, d_C_rr, d_C_ri, d_C_ir, d_C_ii, ffts_addr);
    pipe_barrier(PIPE_ALL);
}
#endif
